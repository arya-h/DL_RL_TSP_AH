{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functioning-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "#print(datetime.now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graphic-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 10, 8, 11]\n",
      "Episode:1 Score:-292.36010517139727\n",
      "[14, 14]\n",
      "Episode:2 Score:-300\n",
      "[10, 6, 11, 5, 6]\n",
      "Episode:3 Score:-293.84491867928017\n",
      "[0, 2, 3, 12, 0]\n",
      "Episode:4 Score:-288.6474797916822\n",
      "[10, 10]\n",
      "Episode:5 Score:-300\n",
      "[7, 6, 1, 6]\n",
      "Episode:6 Score:-297.21293843617065\n",
      "[1, 11, 3, 11]\n",
      "Episode:7 Score:-294.4220982487686\n",
      "[9, 4, 4]\n",
      "Episode:8 Score:-298.72358599465406\n",
      "[11, 1, 14, 5, 0, 9, 8, 2, 12, 6, 8]\n",
      "Episode:9 Score:-269.3129478568221\n",
      "[1, 9, 14, 6, 1]\n",
      "Episode:10 Score:-290.54415897656054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 01:04:54.944233: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "from envTSP import TSPDistCost\n",
    "\n",
    "env = TSPDistCost()\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    path = []\n",
    "    path.append(env.state[0])\n",
    "    while not done:\n",
    "        action = random.randint(0, env.N - 1)\n",
    "        path.append(action)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(path)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "integral-maker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/students/s290510/TFM/eki_tsp/ekit_venv/lib/python3.7/site-packages/keras/callbacks/callbacks.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Input, Concatenate\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weird-justice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 53,647\n",
      "Trainable params: 53,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 01:05:01.800039: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-26 01:05:01.840746: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
      "2022-06-26 01:05:01.846094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563d8fe293a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-26 01:05:01.846133: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env.N +1, env.N)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "secret-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sufficient-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /home/students/s290510/TFM/eki_tsp/ekit_venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: -99.1658\n",
      "3333 episodes - episode_reward: -297.528 [-297.528, -295.421]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: -56.8673\n",
      "2135 episodes - episode_reward: -266.379 [-300.000, -243.135] - loss: 63.891 - mae: 237.689 - mean_q: -222.523\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 94s 9ms/step - reward: -31.1003\n",
      "1262 episodes - episode_reward: -246.442 [-292.927, -231.514] - loss: 5.544 - mae: 251.412 - mean_q: -238.689\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -22.8257\n",
      "953 episodes - episode_reward: -239.472 [-291.822, -231.514] - loss: 5.208 - mae: 244.290 - mean_q: -230.855\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -20.5849\n",
      "871 episodes - episode_reward: -236.380 [-285.518, -226.616] - loss: 4.217 - mae: 244.118 - mean_q: -231.304\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -19.6075\n",
      "831 episodes - episode_reward: -235.961 [-290.340, 65.399] - loss: 4.097 - mae: 243.031 - mean_q: -231.496\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -11.7099\n",
      "801 episodes - episode_reward: -146.191 [-298.266, 80.473] - loss: 70.378 - mae: 199.581 - mean_q: -133.711\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 4.1456\n",
      "721 episodes - episode_reward: 57.580 [-292.087, 84.532] - loss: 103.766 - mae: 159.212 - mean_q: 40.311\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: 3.9316\n",
      "721 episodes - episode_reward: 54.531 [-279.522, 85.739] - loss: 63.011 - mae: 156.528 - mean_q: 47.702\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 4.8574\n",
      "715 episodes - episode_reward: 67.864 [-253.494, 84.770] - loss: 43.267 - mae: 143.388 - mean_q: 44.341\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: 3.2242\n",
      "738 episodes - episode_reward: 43.686 [-296.775, 79.827] - loss: 39.979 - mae: 132.531 - mean_q: 46.353\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 3.4003\n",
      "730 episodes - episode_reward: 46.578 [-284.823, 85.333] - loss: 39.656 - mae: 126.173 - mean_q: 47.629\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 1.5047\n",
      "756 episodes - episode_reward: 19.975 [-293.973, 84.010] - loss: 35.167 - mae: 122.360 - mean_q: 47.299\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 1.1456\n",
      "754 episodes - episode_reward: 15.122 [-295.173, 82.936] - loss: 34.946 - mae: 127.965 - mean_q: 45.439\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 3.0933\n",
      "738 episodes - episode_reward: 41.946 [-292.401, 85.622] - loss: 27.715 - mae: 145.848 - mean_q: 44.536\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 0.1314\n",
      "787 episodes - episode_reward: 1.630 [-297.282, 85.114] - loss: 28.076 - mae: 146.369 - mean_q: 43.535\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 3.0228\n",
      "736 episodes - episode_reward: 41.068 [-286.256, 89.880] - loss: 27.131 - mae: 147.182 - mean_q: 44.298\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 2.1937\n",
      "766 episodes - episode_reward: 28.700 [-298.730, 85.210] - loss: 23.454 - mae: 150.287 - mean_q: 43.489\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 3.5804\n",
      "736 episodes - episode_reward: 48.638 [-295.503, 89.093] - loss: 20.572 - mae: 153.014 - mean_q: 43.424\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: 2.9277\n",
      "done, took 1767.792 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f93ac064550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=65000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10000, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "dqn.compile(optimizer=Adam(learning_rate=0.002), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=200000, visualize=True, verbose=1, callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ekit_venv",
   "language": "python",
   "name": "ekit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
